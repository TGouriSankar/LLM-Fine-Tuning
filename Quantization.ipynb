{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is LLM Fine-Tuning\n",
        "\n",
        "- Fine-Tuning is adapting a pre-trained llm to a specific task or domain\n",
        "- it involves adjusting a small potion of model parameters on more focused dataset\n",
        "- Fine-Tuning customizes output to be more relevant and accurate for your use case\n",
        "\n",
        "The Power of Fine-Tuning\n",
        "\n",
        "- cost-effectiveness\n",
        "- improved performance\n",
        "- data efficiency\n",
        "\n",
        "How Does LLM Fine-Tuning work?\n",
        "\n",
        "- check bellow code\n",
        "\n",
        "Real-World Use Cases\n",
        "- chatbot\n",
        "- content generation\n",
        "- Domain-specifice analysis\n"
      ],
      "metadata": {
        "id": "XayiCdy3ufVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Fine-Tunging\n",
        "\n",
        "Use This Notes:- https://docs.unsloth.ai/"
      ],
      "metadata": {
        "id": "hQB5q52Vx1Qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Post-Training Quantization with bnb"
      ],
      "metadata": {
        "id": "2dA9YH3E0zj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here‚Äôs how you can use BitsAndBytes (bnb) for post-training quantization (no further LoRA fine-tuning) and some knobs to help recover precision:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Post-Training Quantization with bnb\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from bitsandbytes import BitsAndBytesConfig\n",
        "\n",
        "# 1) Choose your model\n",
        "model_name = \"facebook/opt-1.3b\"\n",
        "\n",
        "# 2) Configure bitsandbytes for 4-bit post-training\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                # turn on 4-bit quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",        # use NF4 quant scheme (better than ‚Äúfp4‚Äù)\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # compute in bf16 if GPU allows\n",
        "    bnb_4bit_use_double_quant=True,   # nested quant for extra accuracy\n",
        ")\n",
        "\n",
        "# 3) Load the quantized model directly‚Äîno fine-tuning step\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",      # spreads layers over all GPUs/CPU\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# 4) Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 5) Inference example\n",
        "prompt = \"The quick brown fox\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
        "```\n",
        "\n",
        "**Key bitsandbytes options:**\n",
        "\n",
        "* **`bnb_4bit_quant_type=\"nf4\"`**: ‚ÄúNormal-float4‚Äù often yields better accuracy than plain 4-bit FP.\n",
        "* **`bnb_4bit_use_double_quant=True`**: Applies a two-stage quantization that reduces overall quantization error.\n",
        "* **`bnb_4bit_compute_dtype=torch.bfloat16`**: If you‚Äôre on an Ampere-class or newer GPU, doing the internal rescaling in bf16 instead of fp16 can slightly improve fidelity.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Recovering Precision Without Fine-Tuning\n",
        "\n",
        "1. **Representative Calibration**\n",
        "   Run a small, diverse set of real inputs through the model and compare outputs against FP32. Then adjust the quantization parameters (e.g. switch between `nf4`, `fp4`, or even 8-bit) to find the sweet spot.\n",
        "\n",
        "2. **Layer-Wise Quant Strategy**\n",
        "   Some sensitive layers (e.g. the first and last attention projections) can stay in fp16/bf16, while the bulk of weights are in 4-bit. You can override at load time:\n",
        "\n",
        "   ```python\n",
        "   bnb_config = BitsAndBytesConfig(\n",
        "       load_in_4bit=True,\n",
        "       bnb_4bit_quant_type=\"nf4\",\n",
        "       bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "       bnb_4bit_use_double_quant=True,\n",
        "       # keep first and last Linear layers in fp16:\n",
        "       llm_int8_threshold=6.0,    # higher threshold skips quantization on small layers\n",
        "   )\n",
        "   ```\n",
        "\n",
        "3. **Advanced Recipes**\n",
        "\n",
        "   * **GPTQ**: A ‚Äúgreedy‚Äù per-group quantization that minimizes layer-wise error (via \\[GPTQ-for-LLaMa]).\n",
        "   * **SmoothQuant**: Splits the quant burden between weights and activations to balance error.\n",
        "\n",
        "4. **Mixed Precision Hybrid**\n",
        "   If 4-bit still loses too much, try 8-bit dynamic quant (`load_in_8bit=True`) via the same API:\n",
        "\n",
        "   ```python\n",
        "   bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "   ```\n",
        "\n",
        "   This often hits >90% of FP32 accuracy with a \\~2√ó memory reduction.\n",
        "\n",
        "---\n",
        "\n",
        "### TL;DR\n",
        "\n",
        "* Use **`bnb_4bit_quant_type=\"nf4\"`** + **double quant** + **bf16 compute** for best off-the-shelf 4-bit.\n",
        "* If needed, carve out critical layers to stay in 16-bit or bump to 8-bit.\n",
        "* For large LLMs, consider GPTQ or SmoothQuant wrappers on top of bnb to regain more accuracy‚Äîstill no fine-tuning required.\n"
      ],
      "metadata": {
        "id": "kmoxi3dj1uSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into code, here‚Äôs the short story: **Quantization-Aware Training (QAT)** simulates the effects of low-precision arithmetic during training by inserting ‚Äúfake-quant‚Äù modules into your model‚Äôs forward pass. This lets the model **adapt its weights** to the quantization noise, often recovering **90‚Äì99 %** of full-precision accuracy‚Äîfar better than pure post-training quantization. QAT requires a bit more setup (layer fusion, defining quant-configs, a fine-tuning loop), but it‚Äôs still ‚Äúzero-to-hero‚Äù in under 20 lines of PyTorch once your data loader is ready.\n",
        "\n",
        "---\n",
        "\n",
        "## ## 1. QAT Fundamentals\n",
        "\n",
        "* **FakeQuant modules** clamp and round activations/weights during forward passes, while keeping gradients in full precision. The model ‚Äúsees‚Äù quantization noise and learns to compensate for it ([PyTorch][1]).\n",
        "* You must **fuse** adjacent layers (e.g., Conv+ReLU, Linear+ReLU) before QAT so that quantization points are minimized and more representative of actual inference graphs ([Lei Mao's Log Book][2]).\n",
        "* QAT typically uses the same 8-bit scheme as static PTQ (per-channel weight quant, per-tensor activation quant), but the ranges are **learned** during fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## ## 2. Minimal PyTorch QAT Example\n",
        "\n",
        "Below is a self-contained script to take a pre-trained `resnet18`, fuse it, prepare it for QAT, and fine-tune on CIFAR-10 for just a few epochs. You can adapt it to any image or text model by swapping the backbone and the data loader.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18\n",
        "from torchvision import datasets, transforms\n",
        "import torch.quantization as quant\n",
        "\n",
        "# 1) Data loaders for calibration & training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "\n",
        "# 2) Load & fuse model\n",
        "model_fp32 = resnet18(pretrained=True).eval()\n",
        "model_fp32.fuse_model()  # built-in fusing for ResNet :contentReference[oaicite:2]{index=2}\n",
        "\n",
        "# 3) Specify QAT config: per-channel weights, default activations\n",
        "model_fp32.qconfig = quant.get_default_qat_qconfig('fbgemm')  # Intel server backend :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "# 4) Prepare QAT: insert FakeQuant modules\n",
        "quant.prepare_qat(model_fp32, inplace=True)\n",
        "\n",
        "# 5) Fine-tune with quant noise for a few epochs\n",
        "optimizer = optim.SGD(model_fp32.parameters(), lr=1e-4, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_fp32.train()\n",
        "for epoch in range(3):  # short run for demonstration\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_fp32(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 6) Convert to a true quantized model\n",
        "model_int8 = quant.convert(model_fp32.eval(), inplace=False)\n",
        "\n",
        "# 7) Save or evaluate\n",
        "torch.jit.save(torch.jit.script(model_int8), \"resnet18_qat_int8.pt\")\n",
        "```\n",
        "\n",
        "* After `prepare_qat`, your model‚Äôs forward pass will simulate **8-bit** quantization noise on both weights and activations ([PyTorch][1]).\n",
        "* The final `convert()` swaps FakeQuant modules for real `nnq.Conv2d` / `nnq.Linear` ops that run at INT8 ([Lei Mao's Log Book][2]).\n",
        "\n",
        "---\n",
        "\n",
        "## ## 3. Applying QAT to Large Language Models\n",
        "\n",
        "Recent work (e.g., PyTorch blog on LLama3) shows that QAT on LLMs can recover **96 %** of zero-shot accuracy loss on Hellaswag and **68 %** of perplexity degradation on WikiText compared to PTQ alone ([PyTorch][3]). Hugging Face‚Äôs ü§óOptimum-Intel leverages the same torch-ao QAT APIs under the hood:\n",
        "\n",
        "```bash\n",
        "pip install optimum[openvino]  # brings in NNCF & QAT support\n",
        "```\n",
        "\n",
        "```python\n",
        "from optimum.intel.openvino import OVQuantizer\n",
        "\n",
        "# 1) Instantiate quantizer for QAT\n",
        "quantizer = OVQuantizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "\n",
        "# 2) Prepare model for QAT (wraps torch.quantization under the hood)\n",
        "model = quantizer.prepare_qat(framework=\"pytorch\")\n",
        "\n",
        "# 3) Fine-tune with your SFT or RLHF loop as usual\n",
        "for batch in train_dataloader:\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 4) Convert to an optimized INT8 OpenVINO model\n",
        "quantizer.convert(model)\n",
        "quantizer.save_pretrained(\"./opt-1.3b-q8-qat\")\n",
        "```\n",
        "\n",
        "* This uses Intel‚Äôs **NNCF** toolkit for automated fusing, QConfig selection, and range learning ([Hugging Face][4]).\n",
        "* You can swap in any Hugging Face transformer model by changing the `from_pretrained` argument ([Hugging Face][5]).\n",
        "\n",
        "---\n",
        "\n",
        "## ## 4. Best Practices & Tips\n",
        "\n",
        "| Technique                                                    | Benefit                                                                           |\n",
        "| ------------------------------------------------------------ | --------------------------------------------------------------------------------- |\n",
        "| Per-channel weight quantization                              | Reduces outlier channel errors; essential for conv layers ([PyTorch][1])          |\n",
        "| Fine-tune ‚â•1 epoch with small LR (1e-4)                      | Allows weights to adjust without catastrophic forgetting                          |\n",
        "| Larger batch calibration for activations                     | Better range estimation, especially for language models                           |\n",
        "| Mixed-precision (INT8 weights + FP16 act)                    | Balance between memory saving and numeric stability                               |\n",
        "| Advanced QAT schedulers (e.g., gradual quant noise increase) | Smooth adaptation to quant noise, especially in deep nets ([Weights & Biases][6]) |\n",
        "\n",
        "---\n",
        "\n",
        "**In short:**\n",
        "\n",
        "1. **Fuse** your model.\n",
        "2. **Set** `.qconfig` to a QAT profile.\n",
        "3. **`prepare_qat`**, then **fine-tune** normally.\n",
        "4. **`convert`** to INT8.\n",
        "\n",
        "This process seamlessly integrates into any training loop and, for most CNNs or LLMs, recovers nearly full-precision accuracy‚Äîwithout a single line of extra model architecture code.\n",
        "\n",
        "[1]: https://pytorch.org/docs/stable/quantization.html?utm_source=chatgpt.com \"Quantization ‚Äî PyTorch 2.7 documentation\"\n",
        "[2]: https://leimao.github.io/blog/PyTorch-Quantization-Aware-Training/?utm_source=chatgpt.com \"PyTorch Quantization Aware Training - Lei Mao's Log Book\"\n",
        "[3]: https://pytorch.org/blog/quantization-aware-training/?utm_source=chatgpt.com \"Quantization-Aware Training for Large Language Models with PyTorch\"\n",
        "[4]: https://huggingface.co/docs/optimum/main/en/intel/openvino/optimization?utm_source=chatgpt.com \"Optimization - Hugging Face\"\n",
        "[5]: https://huggingface.co/docs/optimum/en/concept_guides/quantization?utm_source=chatgpt.com \"Quantization - Hugging Face\"\n",
        "[6]: https://wandb.ai/byyoung3/Generative-AI/reports/Quantization-Aware-Training-QAT-A-step-by-step-guide-with-PyTorch--VmlldzoxMTk2NTY2Mw?utm_source=chatgpt.com \"Quantization-Aware Training (QAT): A step-by-step guide with PyTorch\"\n"
      ],
      "metadata": {
        "id": "cRVCYDNQ3RQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quantization\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model = \"\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "XAy18dIOy6vY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}