{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JZsN_tVv5KaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s an enhanced end-to-end recipe for 4-bit (and hybrid 8-bit/4-bit) quantization with BitsAndBytes—including layer-wise overrides, outlier handling, offloading, and post-quant metrics—plus a checklist of what you need to know **before** and **after** quantization.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "We’ll extend your base PTQ snippet to cover:\n",
        "\n",
        "1. **Advanced BitsAndBytesConfig options**\n",
        "\n",
        "   * Layer-wise skipping & fp32 offload\n",
        "   * Outlier threshold tuning (`llm_int8_threshold`) ([Hugging Face][1])\n",
        "   * Mixed-precision hybrids (8-bit/4-bit toggles) ([ApX Machine Learning][2])\n",
        "2. **Pre-quantization checklist**\n",
        "\n",
        "   * Model dtype, supported ops, memory footprint, baseline accuracy.\n",
        "3. **Post-quantization evaluation**\n",
        "\n",
        "   * Memory & VRAM use (`get_memory_footprint`) ([Hugging Face][3])\n",
        "   * Inference latency via `time.time()` loops\n",
        "   * Accuracy/perplexity drop on a validation set\n",
        "4. **Additional recipes**\n",
        "\n",
        "   * GPTQ / SmoothQuant pointers ([Kaggle][4])\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Enhanced BitsAndBytes PTQ Snippet\n",
        "\n",
        "```python\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"facebook/opt-1.3b\"\n",
        "\n",
        "# ─── 1) Advanced BitsAndBytesConfig ────────────────────\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                        # 4-bit weights\n",
        "    bnb_4bit_quant_type=\"nf4\",                # NF4 gives better weight fidelity :contentReference[oaicite:4]{index=4}\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,    # bf16 compute reduces overflow :contentReference[oaicite:5]{index=5}\n",
        "    bnb_4bit_use_double_quant=True,           # nested quantization to claw back ~0.4 bits/param\n",
        "    # Layer-wise overrides:\n",
        "    llm_int8_threshold=6.0,                   # skip INT8 on outlier channels >6 :contentReference[oaicite:6]{index=6}\n",
        "    llm_int8_skip_modules=[\"lm_head\"],        # keep lm_head in full precision if unstable\n",
        "    llm_int8_enable_fp32_cpu_offload=True,    # offload fp32 weights to CPU to fit on GPU :contentReference[oaicite:7]{index=7}\n",
        ")\n",
        "\n",
        "# ─── 2) Load quantized model (no fine-tuning) ─────────────\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",        # auto sharding across GPU/CPU\n",
        "    torch_dtype=\"auto\",       # matches compute dtype\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# ─── 3) Tokenizer ────────────────────────────────────────\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ─── 4) Quick inference test & timing ────────────────────\n",
        "prompt = \"The quick brown fox\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Warm-up\n",
        "_ = model.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "# Time benchmark\n",
        "start = time.time()\n",
        "_ = model.generate(**inputs, max_new_tokens=50)\n",
        "latency = time.time() - start\n",
        "print(f\"Latency for 50 tokens: {latency:.3f}s\")\n",
        "\n",
        "# ─── 5) Save quantized model ─────────────────────────────\n",
        "model.save_pretrained(\"opt-1.3b-4bit-nf4\")\n",
        "tokenizer.save_pretrained(\"opt-1.3b-4bit-nf4\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Pre-Quantization Checklist\n",
        "\n",
        "Before you quantize, ensure you’ve gathered:\n",
        "\n",
        "* **Baseline metrics**\n",
        "\n",
        "  * **FP32 memory footprint** via `model.get_memory_footprint()` ([Hugging Face][3])\n",
        "  * **Inference latency** on representative prompts\n",
        "  * **Accuracy / Perplexity** on your validation set\n",
        "* **Model compatibility**\n",
        "\n",
        "  * Uses supported layers (`nn.Linear`, `nn.Conv`, attention). Unsupported ops fall back to FP32 ([Kaggle][4])\n",
        "* **Hardware constraints**\n",
        "\n",
        "  * GPU architecture (Ampere or newer for bf16)\n",
        "  * Available VRAM vs. model size\n",
        "* **Calibration data** (for static PTQ if you switch from dynamic)\n",
        "\n",
        "  * A small, diverse dataset (100–500 samples) for activation range capture\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Post-Quantization Evaluation\n",
        "\n",
        "After quantization, measure:\n",
        "\n",
        "```python\n",
        "# Memory footprint (bytes)\n",
        "footprint_bytes = model.get_memory_footprint()\n",
        "print(f\"Quantized model footprint: {footprint_bytes/1e9:.2f} GB\")  # :contentReference[oaicite:10]{index=10}\n",
        "\n",
        "# Inference throughput (tokens/sec)\n",
        "tokens = 50\n",
        "throughput = tokens / latency\n",
        "print(f\"Throughput: {throughput:.1f} tokens/s\")\n",
        "\n",
        "# Accuracy / Perplexity drop\n",
        "# (Example using Hugging Face’s `evaluate` library)\n",
        "import evaluate\n",
        "metric = evaluate.load(\"perplexity\")\n",
        "eval_data = [\"Hello world!\", \"Quantization is cool.\"]  # your validation split\n",
        "ppl_scores = []\n",
        "for txt in eval_data:\n",
        "    inpt = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model(**inpt, labels=inpt[\"input_ids\"])\n",
        "    ppl_scores.append(out.loss.item())\n",
        "print(\"Avg perplexity:\", float(torch.exp(torch.tensor(ppl_scores))))\n",
        "```\n",
        "\n",
        "Compare these against your FP32 baseline to quantify trade-offs.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Additional Recipes & Next Steps\n",
        "\n",
        "* **8-bit dynamic quant**:\n",
        "\n",
        "  ```python\n",
        "  bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "  ```\n",
        "\n",
        "  Hits \\~90 % FP32 accuracy with 2× memory reduction ([ApX Machine Learning][2]).\n",
        "\n",
        "* **GPTQ** (per-group quant) and **SmoothQuant** wrappers can further reduce error without retraining ([Kaggle][4]).\n",
        "\n",
        "* **Quantization-Aware Training (QAT)** if you need >99 % accuracy recovery—insert fake-quant modules and fine-tune for a few epochs.\n",
        "\n",
        "---\n",
        "\n",
        "By following these patterns, you’ll have a reproducible pipeline to quantize **and** validate your large-scale models—maximizing efficiency while keeping accuracy loss in check.\n",
        "\n",
        "[1]: https://huggingface.co/docs/transformers/en/quantization/bitsandbytes?utm_source=chatgpt.com \"bitsandbytes - Hugging Face\"\n",
        "[2]: https://apxml.com/courses/quantized-llm-deployment/chapter-2-implementing-llm-quantization-toolkits/quantization-hf-transformers-accelerate?utm_source=chatgpt.com \"Quantization with Hugging Face Transformers and Accelerate\"\n",
        "[3]: https://huggingface.co/docs/transformers/main/en//quantization?utm_source=chatgpt.com \"Quantization - Hugging Face\"\n",
        "[4]: https://www.kaggle.com/code/enesbeinci/how-to-work-with-mistral7b?utm_source=chatgpt.com \"how-to-work-with-mistral7b - Kaggle\"\n"
      ],
      "metadata": {
        "id": "ccnjI38V5L2Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5fuoMmM48r6"
      },
      "outputs": [],
      "source": []
    }
  ]
}